{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ada35-b1dd-4c77-8ff8-43e8e384e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data_eas = pd.read_csv('C:/Users/seraf/Downloads/EAS MACHINE LEARNING/EAS LSTM/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d71563-58ca-49fb-98b7-be89d7bb6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd05d2-546b-4f42-930e-a7ee8f5a0b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import FastMarkerCluster\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "# Generate random coordinates (replace with your data)\n",
    "\n",
    "lats = data_eas['Latitude']\n",
    "lons = data_eas['Longitude']\n",
    "\n",
    "# Combine latitude and longitude into a list of lists\n",
    "#heat_data = list(zip(lats, lons))\n",
    "\n",
    "# Add a heatmap layer\n",
    "#HeatMap(heat_data, radius=15).add_to(mymap)\n",
    "\n",
    "# Create a DataFrame from the coordinates\n",
    "df = pd.DataFrame({'lat': lats, 'lon': lons})\n",
    "\n",
    "# Downsample the data to reduce the number of points\n",
    "downsampled_df = df.sample(frac=0.3)  # Adjust the fraction as needed\n",
    "\n",
    "# Create a Folium map centered around the mean of coordinates\n",
    "map_center = [np.mean(downsampled_df['lat']), np.mean(downsampled_df['lon'])]\n",
    "mymap = folium.Map(location=map_center, zoom_start=9)\n",
    "\n",
    "# Add downsampled data points to the map\n",
    "for _, row in downsampled_df.iterrows():\n",
    "    folium.CircleMarker(location=[row['lat'], row['lon']], radius=1, color='blue', fill=True, fill_color='blue').add_to(mymap)\n",
    "\n",
    "# Display the map\n",
    "display(mymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf5fb4-98bb-4200-99e9-61574f73ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install wandb -qqq\n",
    "#import wandb\n",
    "# Log in to your W&B account\n",
    "#wandb.login()\n",
    "import sys\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D, Flatten\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, EarlyStopping\n",
    "from cond_rnn import ConditionalRecurrent\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Import data\n",
    "data_path = 'C:/Users/seraf/Downloads/EAS MACHINE LEARNING/data/data_seq/'\n",
    "x_train = np.load(data_path + '/x_train.npz')\n",
    "x_test = np.load(data_path + '/x_test.npy')\n",
    "x_valid = np.load(data_path + '/x_valid.npy')\n",
    "y_train = np.load(data_path + '/y_train.npy')\n",
    "y_test = np.load(data_path + '/y_test.npy')\n",
    "y_valid = np.load(data_path + '/y_valid.npy')\n",
    "\n",
    "x_train = np.asarray(x_train['arr_0']).astype(np.float32)\n",
    "# Create inputs sets\n",
    "\n",
    "#0:'Max_Draught'\n",
    "#1: 'Latitude'\n",
    "#2: 'Longitude'\n",
    "#3: 'Speed_over_Ground'\n",
    "#4: 'COG_cos'\n",
    "#5: 'COG_sin'\n",
    "#6: 'TH_cos'\n",
    "#7: 'TH_sin'\n",
    "#8: 'Navigational_Status_0.0'\n",
    "#9: 'Navigational_Status_1.0'\n",
    "#10: 'Navigational_Status_2.0'\n",
    "#11: 'Navigational_Status_3.0'\n",
    "#12: 'Navigational_Status_4.0'\n",
    "#13: 'Na13vigational_Status_5.0'\n",
    "#14: 'Nav14igational_Status_8.0'\n",
    "#15: 'Navi15gational_Status_15.0'\n",
    "#16: 'GT'\n",
    "#17: 'DWT'\n",
    "#18: 'LOA'\n",
    "#19: 'BEAM'\n",
    "#20: 'VesselTypeB_Cargo'\n",
    "#21: 'VesselTypeB_Tanker'\n",
    "#22: 'Age'\n",
    "#23: 'current_utotal' \n",
    "#24: 'current_vtotal'\n",
    "#25: 'wind_u10'\n",
    "#26: 'wind_v10'\n",
    "#27: 'mwd' \n",
    "#28: 'mwp' \n",
    "#29: 'swh'\n",
    "#30: 'sst'  \n",
    "#31: 'Origin_Lat'\n",
    "#32: 'Origin_Lon'\n",
    "#33: 'acc_dist' \n",
    "#34: 'acc_time_hours'\n",
    "#35: 'leg_distance'\n",
    "#36: 'leg_speed'\n",
    "#37: 'leg_elapsed_time_hours'\n",
    "#38: 'remaining_distance'\n",
    "\n",
    "# Data split of sequence and non sequence variables\n",
    "\n",
    "### Inputs set: All varaibles -- AIS + Vessel particulars + Weather + Crafted features ###\n",
    "###########################################################################################\n",
    "#sequence variables\n",
    "x_train_nn_cc= np.delete(x_train,[16,17,18,19,20,21,22,31,32],2)\n",
    "x_test_nn_cc= np.delete(x_test,[16,17,18,19,20,21,22,31,32],2)\n",
    "x_valid_nn_cc= np.delete(x_valid,[16,17,18,19,20,21,22,31,32],2)\n",
    "\n",
    "#non sequence variables \n",
    "x_train_cc_split= np.delete(x_train,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)\n",
    "x_test_cc_split= np.delete(x_test,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)\n",
    "x_valid_cc_split= np.delete(x_valid,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)\n",
    "\n",
    "### Inputs set: AIS + Vessel particulars ###\n",
    "###################################################\n",
    "#sequence variables\n",
    "\n",
    "#x_train_nn_cc= np.delete(x_train,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30,33,34,35,36,38],2)\n",
    "#x_test_nn_cc= np.delete(x_test,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30,33,34,35,36,38],2)\n",
    "#x_valid_nn_cc= np.delete(x_valid,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30,33,34,35,36,38],2)\n",
    "\n",
    "#non sequence variables \n",
    "\n",
    "#x_train_cc_split= np.delete(x_train,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)\n",
    "#x_test_cc_split= np.delete(x_test,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)\n",
    "#x_valid_cc_split= np.delete(x_valid,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)\n",
    "\n",
    "### Inputs set: AIS + Vessel particulars ###\n",
    "###################################################\n",
    "#sequence variables\n",
    "\n",
    "#x_train_nn_cc= np.delete(x_train,[16,17,18,19,20,21,22,31,32,33,34,35,36,38],2)\n",
    "#x_test_nn_cc= np.delete(x_test,[16,17,18,19,20,21,22,31,32,33,34,35,36,38],2)\n",
    "#x_valid_nn_cc= np.delete(x_valid,[16,17,18,19,20,21,22,31,32,33,34,35,36,38],2)\n",
    "\n",
    "#non sequence variables \n",
    "#x_train_cc_split= np.delete(x_train,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)\n",
    "#x_test_cc_split= np.delete(x_test,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)\n",
    "#x_valid_cc_split= np.delete(x_valid,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38,31,32],2)\n",
    "\n",
    "### Inputs set: AIS + Vessel particulars ###\n",
    "###################################################\n",
    "#sequence variables\n",
    "\n",
    "#x_train_nn_cc= np.delete(x_train,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30],2)\n",
    "#x_test_nn_cc= np.delete(x_test,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30],2)\n",
    "#x_valid_nn_cc= np.delete(x_valid,[16,17,18,19,20,21,22,31,32,23,24,25,26,27,28,29,30],2)\n",
    "\n",
    "#non sequence variables \n",
    "\n",
    "#x_train_cc_split= np.delete(x_train,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)\n",
    "#x_test_cc_split= np.delete(x_test,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)\n",
    "#x_valid_cc_split= np.delete(x_valid,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_valid shape:', x_valid.shape)\n",
    "print('y_valid shape:', y_valid.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('')\n",
    "print('x_train_nn_cc shape:', x_train_nn_cc.shape)\n",
    "print('x_valid_nn_cc shape:', x_valid_nn_cc.shape)\n",
    "print('x_test_nn_cc shape:', x_test_nn_cc.shape)\n",
    "print('')\n",
    "print('x_train_cc shape:', x_train_cc_split.shape)\n",
    "print('x_valid_cc shape:', x_valid_cc_split.shape)\n",
    "print('x_test_cc shape:', x_test_cc_split.shape)\n",
    "\n",
    "n_features_nn_cc = x_train_nn_cc.shape[2]\n",
    "n_features_cc = x_train_cc_split.shape[2]\n",
    "steps = x_train.shape[1]\n",
    "print('n_features_nn_cc = ', n_features_nn_cc)\n",
    "print('n_features_cc = ', n_features_cc)\n",
    "print('steps = ', steps)\n",
    "\n",
    "\n",
    "test_seq_len = np.load(data_path + '/test_sequence_length.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da0583-56c8-4862-80bb-16a0a2b6acbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df87f8-9aab-42f2-9397-968ca90793c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get non sequence data into the right shape\n",
    "def get_cc_data(data):\n",
    "  X = []\n",
    "  for i in range(data.shape[0]):\n",
    "    x = data[i,0,:]\n",
    "    x = x.reshape (1,9) \n",
    "    X.append(x)\n",
    "  X = np.array(X).reshape(data.shape[0],data.shape[2])\n",
    "  return X  \n",
    "\n",
    "x_train_cc = get_cc_data(x_train_cc_split)\n",
    "x_valid_cc = get_cc_data(x_valid_cc_split)\n",
    "x_test_cc = get_cc_data(x_test_cc_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7411e9-e851-46dd-b1a6-7fc65480755f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "def train(n):\n",
    "    # Default values for hyper-parameters we're going to sweep over\n",
    "\n",
    "    config_try = {\n",
    "        'epochs': n,\n",
    "        'batch_size': 64,\n",
    "        'log_learning_rate': -2.4773670403758667,\n",
    "        'lstm_size': 50,\n",
    "        'dropout_out': 0.2,\n",
    "        'dropout_inp': 0.2,\n",
    "        'dropout_rec':0.1,\n",
    "        'seed': 40\n",
    "    }\n",
    "\n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "    config = config_try\n",
    "    \n",
    "    # Define the model architecture \n",
    "    checkpoint_filepath = f\"folds.hdf5\"\n",
    "    i = Input(shape=(steps, n_features_nn_cc))\n",
    "    ic = Input(shape=(n_features_cc,))\n",
    "    m = ConditionalRecurrent(LSTM(config['lstm_size'], return_sequences=True, dropout=config['dropout_out'], recurrent_dropout=config['dropout_rec']))([i, ic])\n",
    "    m = Dropout(config['dropout_rec'])(m)\n",
    "    m = TimeDistributed(Dense(1, activation = 'relu'))(m) \n",
    "    model = Model([i, ic], m)  \n",
    "    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1, mode=\"min\", restore_best_weights=True)\n",
    "    sv = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "        options=None\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate = 10**config['log_learning_rate']), loss='mse', metrics=['mae'])\n",
    "\n",
    "         # Train the model and get the training history\n",
    "    history = model.fit([x_train_nn_cc, x_train_cc], y_train, batch_size=config['batch_size'],\n",
    "                        epochs=config['epochs'],\n",
    "                        shuffle=False,\n",
    "                        validation_data=([x_valid_nn_cc, x_valid_cc], y_valid),\n",
    "                        callbacks=[lr,es])\n",
    "    train_loss, train_mae = model.evaluate([x_train_nn_cc, x_train_cc], y_train, verbose=2)\n",
    "    valid_loss, valid_mae = model.evaluate([x_valid_nn_cc, x_valid_cc], y_valid, verbose=2)\n",
    "    test_loss, test_mae = model.evaluate([x_test_nn_cc, x_test_cc], y_test, verbose=2)\n",
    "    \n",
    "    model.save(\"C:/Users/seraf/OneDrive/Documents/lstm_EAS.h5\")\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Call the train function\n",
    "trained_model, training_history = train(300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7469ab4-0e44-42d4-8294-bbb2d08e144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the training loss over epochs\n",
    "\n",
    "plt.plot(training_history.history['loss'])\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation loss over epochs\n",
    "plt.plot(training_history.history['val_loss'])\n",
    "plt.title('Validation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('val_loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52dc2e-6f4e-43a1-a63a-5947da7e9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot y_true vs y_predictions from test set\n",
    "def plot_y_true_y_pred(name, model, x_test, y_test):\n",
    "  test_predictions = model.predict(x_test)\n",
    "\n",
    "  plt.figure(figsize=[2, 2], dpi=150)\n",
    "  a = plt.axes(aspect='equal')\n",
    "\n",
    "  plt.scatter(y_test, test_predictions, s=0.1)\n",
    "  plt.xticks(fontsize=5)\n",
    "  plt.yticks(fontsize=5)\n",
    "  plt.xlabel('True Values [hour]', fontsize=7)\n",
    "  plt.ylabel('Predictions [hour]', fontsize=7)\n",
    "  plt.title(name, fontsize=8)\n",
    "  lims = [0, 10]\n",
    "  plt.xlim(lims)\n",
    "  plt.ylim(lims)\n",
    "  _ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1f143-bfff-47f0-8fea-da0fb8e7f4d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSTM Dengan Cuaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f757a6-ce5b-43c2-9fe5-84e6e26ac25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the models\n",
    "models_path = data_path = 'C:/Users/seraf/OneDrive/Documents/'\n",
    "lstm_EAS = keras.models.load_model(models_path + 'lstm_EAS.h5', custom_objects={'ConditionalRecurrent': ConditionalRecurrent})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b88d61-cbf1-4fb8-9b27-daee5ba7d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_true_y_pred('LSTM', lstm_EAS, [x_test_nn_cc, x_test_cc], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22cd97-fcb9-4dde-b429-a1f063aa3904",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate the mse per obseravtion of sequence models for comparison with non sequence models \n",
    "def mse_obs(x_data_nn_cc, x_data_cc, y_data, model, seq_len):\n",
    "  \"\"\"\n",
    "    Calculates the mse per observation of the sequence models\n",
    "    \n",
    "    Arguments:\n",
    "    x_data_nn_cc -- sequential data\n",
    "    x_data_cc -- non sequential data\n",
    "    y_data -- true target\n",
    "    model -- predictive model\n",
    "    seq_len -- the true sequence length of the example\n",
    "\n",
    "    Returns:\n",
    "    squared error -- list of mse per observation\n",
    "    MSE -- mean squared error\n",
    "    \"\"\"\n",
    "    \n",
    "  squared_error = []\n",
    "\n",
    "  for i in range(x_data_nn_cc.shape[0]):\n",
    "    y = y_data[i].reshape(1,steps,1)\n",
    "    z = seq_len[i]\n",
    "    y_hat = model.predict([x_data_nn_cc[i].reshape(1,steps,n_features_nn_cc), x_data_cc[i].reshape(1,n_features_cc)])\n",
    "    error = np.square(y[:,:z,:] - y_hat[:,:z,:])\n",
    "    error = error.reshape(z)\n",
    "    error = error.tolist()\n",
    "    squared_error.extend(error)\n",
    " \n",
    "  return np.mean(squared_error), squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fd2ad-559c-4575-bc86-2c58e5d432a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#calculate the MAE per obseravtion of sequence models for comparison with non sequence models \n",
    "def mae_obs(x_data_nn_cc, x_data_cc, y_data, model, seq_len) :\n",
    "  \"\"\"\n",
    "    Calculates the mse per observation of the sequence models\n",
    "    \n",
    "    Arguments:\n",
    "    x_data_nn_cc -- sequential data\n",
    "    x_data_cc -- non sequential data\n",
    "    y_data -- true target\n",
    "    model -- predictive model\n",
    "    seq_len -- the true sequence length of the example\n",
    "\n",
    "    Returns:\n",
    "    absolute error -- list of mse per observation\n",
    "    MAE -- mean absolute error\n",
    "    \"\"\"\n",
    "  absolute_error =[]\n",
    "  for i in range(x_data_nn_cc.shape[0]):\n",
    "    y = y_data[i].reshape(1,steps,1)\n",
    "    z = seq_len[i]\n",
    "    y_hat = model.predict([x_data_nn_cc[i].reshape(1,steps,n_features_nn_cc),x_data_cc[i].reshape(1,n_features_cc)])\n",
    "    error = np.abs(y[:,:z,:] - y_hat[:,:z,:])\n",
    "    error = error.reshape(z)\n",
    "    error = error.tolist()\n",
    "    absolute_error.extend(error)\n",
    "  return np.mean(absolute_error), absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1c408-7db2-49e0-9cf6-808ba95f2fb3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate mse for sequence models\n",
    "lstm_cuaca_mse_obs = mse_obs(x_test_nn_cc, x_test_cc, y_test, lstm_EAS, test_seq_len)\n",
    "\n",
    "\n",
    "\n",
    "#calculate mae for sequence models\n",
    "lstm_cuaca_mae_obs = mae_obs(x_test_nn_cc, x_test_cc, y_test, lstm_EAS, test_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6431c-350f-4aa0-a91a-d63e3bf5f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('The performance metrics on the test set are:')\n",
    "print('')\n",
    "print('LSTM_1:')\n",
    "print('The MSE considering errors per observation:', lstm_cuaca_mse_obs[0])\n",
    "\n",
    "print('')\n",
    "\n",
    "print('The performance metrics on the test set are:')\n",
    "print('')\n",
    "print('LSTM_1:')\n",
    "print('MAE considering errors per observation:', lstm_cuaca_mae_obs[0] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11a214-567c-4955-a69c-f4af0d1e0d0d",
   "metadata": {},
   "source": [
    "## Tanpa Cuaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d925895-802d-4bc3-af13-10eacf748485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install wandb -qqq\n",
    "#import wandb\n",
    "# Log in to your W&B account\n",
    "#wandb.login()\n",
    "import sys\n",
    "import numpy as np \n",
    "import math\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D, Flatten\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, EarlyStopping\n",
    "from cond_rnn import ConditionalRecurrent\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Import data\n",
    "data_path = 'C:/Users/seraf/Downloads/EAS MACHINE LEARNING/data/data_seq/'\n",
    "x_train = np.load(data_path + '/x_train.npz')\n",
    "x_test = np.load(data_path + '/x_test.npy')\n",
    "x_valid = np.load(data_path + '/x_valid.npy')\n",
    "y_train = np.load(data_path + '/y_train.npy')\n",
    "y_test = np.load(data_path + '/y_test.npy')\n",
    "y_valid = np.load(data_path + '/y_valid.npy')\n",
    "\n",
    "x_train = np.asarray(x_train['arr_0']).astype(np.float32)\n",
    "\n",
    "\n",
    "#sequence variables\n",
    "x_train_nn_cc= np.delete(x_train,[16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],2)\n",
    "x_test_nn_cc= np.delete(x_test,[16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],2)\n",
    "x_valid_nn_cc= np.delete(x_valid,[16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],2)\n",
    "\n",
    "#non sequence variables \n",
    "x_train_cc_split= np.delete(x_train,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)\n",
    "x_test_cc_split= np.delete(x_test,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)\n",
    "x_valid_cc_split= np.delete(x_valid,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,24,25,26,27,28,29,30,33,34,35,36,37,38],2)\n",
    "\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_valid shape:', x_valid.shape)\n",
    "print('y_valid shape:', y_valid.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('')\n",
    "print('x_train_nn_cc shape:', x_train_nn_cc.shape)\n",
    "print('x_valid_nn_cc shape:', x_valid_nn_cc.shape)\n",
    "print('x_test_nn_cc shape:', x_test_nn_cc.shape)\n",
    "print('')\n",
    "print('x_train_cc shape:', x_train_cc_split.shape)\n",
    "print('x_valid_cc shape:', x_valid_cc_split.shape)\n",
    "print('x_test_cc shape:', x_test_cc_split.shape)\n",
    "\n",
    "n_features_nn_cc = x_train_nn_cc.shape[2]\n",
    "n_features_cc = x_train_cc_split.shape[2]\n",
    "steps = x_train.shape[1]\n",
    "print('n_features_nn_cc = ', n_features_nn_cc)\n",
    "print('n_features_cc = ', n_features_cc)\n",
    "print('steps = ', steps)\n",
    "\n",
    "\n",
    "test_seq_len = np.load(data_path + '/test_sequence_length.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5f303-b615-4d12-ba5a-d62ad708b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get non sequence data into the right shape\n",
    "def get_cc_data(data):\n",
    "  X = []\n",
    "  for i in range(data.shape[0]):\n",
    "    x = data[i,0,:]\n",
    "    x = x.reshape (1,9) \n",
    "    X.append(x)\n",
    "  X = np.array(X).reshape(data.shape[0],data.shape[2])\n",
    "  return X  \n",
    "\n",
    "x_train_cc = get_cc_data(x_train_cc_split)\n",
    "x_valid_cc = get_cc_data(x_valid_cc_split)\n",
    "x_test_cc = get_cc_data(x_test_cc_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3beed9-255d-48cc-8110-54222ad6c260",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "def train(n):\n",
    "    # Default values for hyper-parameters we're going to sweep over\n",
    "\n",
    "    config_try = {\n",
    "        'epochs': n,\n",
    "        'batch_size': 64,\n",
    "        'log_learning_rate': -2.4773670403758667,\n",
    "        'lstm_size': 50,\n",
    "        'dropout_out': 0.2,\n",
    "        'dropout_inp': 0.2,\n",
    "        'dropout_rec':0.1,\n",
    "        'seed': 40\n",
    "    }\n",
    "\n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "    config = config_try\n",
    "    \n",
    "    # Define the model architecture \n",
    "    checkpoint_filepath = f\"folds.hdf5\"\n",
    "    i = Input(shape=(steps, n_features_nn_cc))\n",
    "    ic = Input(shape=(n_features_cc,))\n",
    "    m = ConditionalRecurrent(LSTM(config['lstm_size'], return_sequences=True, dropout=config['dropout_out'], recurrent_dropout=config['dropout_rec']))([i, ic])\n",
    "    m = Dropout(config['dropout_rec'])(m)\n",
    "    m = TimeDistributed(Dense(1, activation = 'relu'))(m) \n",
    "    model = Model([i, ic], m)  \n",
    "    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1, mode=\"min\", restore_best_weights=True)\n",
    "    sv = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "        options=None\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate = 10**config['log_learning_rate']), loss='mse', metrics=['mae'])\n",
    "\n",
    "         # Train the model and get the training history\n",
    "    history = model.fit([x_train_nn_cc, x_train_cc], y_train, batch_size=config['batch_size'],\n",
    "                        epochs=config['epochs'],\n",
    "                        shuffle=False,\n",
    "                        validation_data=([x_valid_nn_cc, x_valid_cc], y_valid),\n",
    "                        callbacks=[lr,es])\n",
    "    train_loss, train_mae = model.evaluate([x_train_nn_cc, x_train_cc], y_train, verbose=2)\n",
    "    valid_loss, valid_mae = model.evaluate([x_valid_nn_cc, x_valid_cc], y_valid, verbose=2)\n",
    "    test_loss, test_mae = model.evaluate([x_test_nn_cc, x_test_cc], y_test, verbose=2)\n",
    "    \n",
    "    model.save(\"C:/Users/seraf/OneDrive/Documents/lstm_EAS_tnp_cuaca.h5\")\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Call the train function\n",
    "trained_model, training_history = train(300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ecdf3-7157-4904-892f-de3f7dfb4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the training loss over epochs\n",
    "\n",
    "plt.plot(training_history.history['loss'])\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation loss over epochs\n",
    "plt.plot(training_history.history['val_loss'])\n",
    "plt.title('Validation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('val_loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5372121-be03-41c3-9ebe-13f1c5b705a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the models\n",
    "models_path = data_path = 'C:/Users/seraf/OneDrive/Documents/'\n",
    "lstm_EAS_tnp_cuaca = keras.models.load_model(models_path + 'lstm_EAS_tnp_cuaca.h5', custom_objects={'ConditionalRecurrent': ConditionalRecurrent})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a57097-3969-49f7-a267-a9cd9347c171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_y_true_y_pred('LSTM Tanpa Cuaca', lstm_EAS_tnp_cuaca, [x_test_nn_cc, x_test_cc], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966eb02-b9c9-42b9-8777-6bc6c8a59e90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate the mse per obseravtion of sequence models for comparison with non sequence models \n",
    "def mse_obs(x_data_nn_cc, x_data_cc, y_data, model, seq_len):\n",
    "  \"\"\"\n",
    "    Calculates the mse per observation of the sequence models\n",
    "    \n",
    "    Arguments:\n",
    "    x_data_nn_cc -- sequential data\n",
    "    x_data_cc -- non sequential data\n",
    "    y_data -- true target\n",
    "    model -- predictive model\n",
    "    seq_len -- the true sequence length of the example\n",
    "\n",
    "    Returns:\n",
    "    squared error -- list of mse per observation\n",
    "    MSE -- mean squared error\n",
    "    \"\"\"\n",
    "    \n",
    "  squared_error = []\n",
    "\n",
    "  for i in range(x_data_nn_cc.shape[0]):\n",
    "    y = y_data[i].reshape(1,steps,1)\n",
    "    z = seq_len[i]\n",
    "    y_hat = model.predict([x_data_nn_cc[i].reshape(1,steps,n_features_nn_cc), x_data_cc[i].reshape(1,n_features_cc)])\n",
    "    error = np.square(y[:,:z,:] - y_hat[:,:z,:])\n",
    "    error = error.reshape(z)\n",
    "    error = error.tolist()\n",
    "    squared_error.extend(error)\n",
    " \n",
    "  return np.mean(squared_error), squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405f813-9821-42e8-ad4f-a351670a363b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#calculate the MAE per obseravtion of sequence models for comparison with non sequence models \n",
    "def mae_obs(x_data_nn_cc, x_data_cc, y_data, model, seq_len) :\n",
    "  \"\"\"\n",
    "    Calculates the mse per observation of the sequence models\n",
    "    \n",
    "    Arguments:\n",
    "    x_data_nn_cc -- sequential data\n",
    "    x_data_cc -- non sequential data\n",
    "    y_data -- true target\n",
    "    model -- predictive model\n",
    "    seq_len -- the true sequence length of the example\n",
    "\n",
    "    Returns:\n",
    "    absolute error -- list of mse per observation\n",
    "    MAE -- mean absolute error\n",
    "    \"\"\"\n",
    "  absolute_error =[]\n",
    "  for i in range(x_data_nn_cc.shape[0]):\n",
    "    y = y_data[i].reshape(1,steps,1)\n",
    "    z = seq_len[i]\n",
    "    y_hat = model.predict([x_data_nn_cc[i].reshape(1,steps,n_features_nn_cc),x_data_cc[i].reshape(1,n_features_cc)])\n",
    "    error = np.abs(y[:,:z,:] - y_hat[:,:z,:])\n",
    "    error = error.reshape(z)\n",
    "    error = error.tolist()\n",
    "    absolute_error.extend(error)\n",
    "  return np.mean(absolute_error), absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ac6fa-7a60-46d2-88a1-c171c3e7cd16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate mse for sequence models\n",
    "lstm_tdk_cuaca_mse_obs = mse_obs(x_test_nn_cc, x_test_cc, y_test, lstm_EAS_tnp_cuaca, test_seq_len)\n",
    "\n",
    "\n",
    "\n",
    "#calculate mae for sequence models\n",
    "lstm_tdk_cuaca_mae_obs = mae_obs(x_test_nn_cc, x_test_cc, y_test, lstm_EAS_tnp_cuaca, test_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec575f-06e3-4eaf-a3a0-8be2893041e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('The performance metrics on the test set are:')\n",
    "print('')\n",
    "print('LSTM_1:')\n",
    "print('The MSE considering errors per observation:', lstm_tdk_cuaca_mse_obs[0])\n",
    "\n",
    "print('')\n",
    "\n",
    "print('The performance metrics on the test set are:')\n",
    "print('')\n",
    "print('LSTM_1:')\n",
    "print('MAE considering errors per observation:', lstm_tdk_cuaca_mae_obs[0] )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
